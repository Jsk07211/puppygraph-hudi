# Use Hive catalog via HMS
spark.sql.catalogImplementation=hive
spark.hadoop.hive.metastore.uris=thrift://metastore:9083

# Hudi integration
spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension
spark.sql.catalog.spark_catalog=org.apache.spark.sql.hudi.catalog.HoodieCatalog

# Auto-resolve dependencies + writable Ivy cache in container
spark.jars.packages=org.apache.hudi:hudi-spark3.5-bundle_2.12:0.15.0,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262
spark.jars.ivy=/tmp/.ivy2

# MinIO (S3A)
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.access.key=admin
spark.hadoop.fs.s3a.secret.key=password
spark.hadoop.fs.s3a.connection.ssl.enabled=false
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider

# Local dev niceties
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.sql.shuffle.partitions=4
